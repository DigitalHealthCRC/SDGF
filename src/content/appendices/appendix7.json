{
  "id": 7,
  "title": "Appendix 7 - De-identification Techniques and Privacy Evaluation",
  "purpose": "Reference guidance on de-identification methods and how to evaluate re-identification risks in synthetic data projects.",
  "type": "read",
  "component": "ReadOnlyContent",
  "template": false,
  "personaVisibility": [
    "custodian",
    "requestor",
    "scientist"
  ],
  "exportKey": "appendix7_deid_reference",
  "body": [
    "De-identification encompasses techniques that reduce the likelihood that an individual can be reasonably identified in a dataset. Removal or modification of direct identifiers alone is insufficient; residual risks must be evaluated in context.",
    "Privacy evaluation should focus on realistic adversarial knowledge via quasi-identifiers. Evaluate all records, avoid stand-alone similarity metrics for privacy claims, and be cautious interpreting differential privacy budgets without empirical checks.",
    "Differential privacy provides formal guarantees when correctly applied, but its parameters (e.g., epsilon) do not automatically translate into empirical privacy without careful calibration and validation.",
    "Common misconceptions to avoid include assuming synthetic data is inherently private or that record-level similarity directly indicates privacy risk. Context-driven evaluation is required.",
    "Use this appendix during Step 4 to guide the Re-identification Risk Assessment and to select proportionate techniques that balance privacy and utility."
  ]
}