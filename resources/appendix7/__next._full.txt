1:"$Sreact.fragment"
2:I[76526,["/SDGF/_next/static/chunks/480617ecc94242dd.js"],"ProgressProvider"]
3:"$Sreact.suspense"
4:I[13474,["/SDGF/_next/static/chunks/480617ecc94242dd.js"],"Navigation"]
5:I[39756,["/SDGF/_next/static/chunks/ff1a16fafef87110.js","/SDGF/_next/static/chunks/247eb132b7f7b574.js"],"default"]
6:I[37457,["/SDGF/_next/static/chunks/ff1a16fafef87110.js","/SDGF/_next/static/chunks/247eb132b7f7b574.js"],"default"]
8:I[97367,["/SDGF/_next/static/chunks/ff1a16fafef87110.js","/SDGF/_next/static/chunks/247eb132b7f7b574.js"],"OutletBoundary"]
a:I[97367,["/SDGF/_next/static/chunks/ff1a16fafef87110.js","/SDGF/_next/static/chunks/247eb132b7f7b574.js"],"ViewportBoundary"]
c:I[97367,["/SDGF/_next/static/chunks/ff1a16fafef87110.js","/SDGF/_next/static/chunks/247eb132b7f7b574.js"],"MetadataBoundary"]
e:I[68027,["/SDGF/_next/static/chunks/ff1a16fafef87110.js","/SDGF/_next/static/chunks/247eb132b7f7b574.js"],"default"]
:HL["/SDGF/_next/static/chunks/ceeba552ba2b4033.css","style"]
:HL["/SDGF/_next/static/chunks/58e45d4f177e378e.css","style"]
:HL["/SDGF/_next/static/media/68d403cf9f2c68c5-s.p.f9f15f61.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/SDGF/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/SDGF/_next/static/media/83afe278b6a6bb3c-s.p.3a6ba036.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/SDGF/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/SDGF/print.css","style",{"media":"print"}]
:HL["/SDGF/assets/chatbot/chatbot-theme.css","style"]
0:{"P":null,"b":"K6tTCbOX2BOgysBDLQrQQ","c":["","resources","appendix7"],"q":"","i":false,"f":[[["",{"children":["resources",{"children":["(appendices)",{"children":[["id","appendix7","d"],{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/SDGF/_next/static/chunks/ceeba552ba2b4033.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/SDGF/_next/static/chunks/58e45d4f177e378e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/SDGF/_next/static/chunks/480617ecc94242dd.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"stylesheet","href":"/SDGF/print.css","media":"print"}],["$","link",null,{"rel":"icon","href":"/SDGF/favicon.png","sizes":"192x192"}],["$","link",null,{"rel":"apple-touch-icon","href":"/SDGF/favicon.png"}]]}],["$","body",null,{"className":"inter_5972bc34-module__OU16Qa__className","children":[["$","$L2",null,{"children":["$","$3",null,{"fallback":null,"children":[["$","$L4",null,{}],["$","main",null,{"className":"min-h-screen","children":["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}]}],["$","link",null,{"rel":"stylesheet","href":"/SDGF/assets/chatbot/chatbot-theme.css"}],["$","script",null,{"src":"https://cdn.platform.openai.com/deployments/chatkit/chatkit.js","async":true}],["$","script",null,{"src":"/SDGF/assets/chatbot/chatkit.js","defer":true}]]}]]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L7",[["$","script","script-0",{"src":"/SDGF/_next/static/chunks/34ee24179ca0df5c.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/SDGF/_next/static/chunks/3708046326082a93.js","async":true,"nonce":"$undefined"}],["$","script","script-2",{"src":"/SDGF/_next/static/chunks/3c1c2c6d31fa05ec.js","async":true,"nonce":"$undefined"}]],["$","$L8",null,{"children":["$","$3",null,{"name":"Next.MetadataOutlet","children":"$@9"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$La",null,{"children":"$@b"}],["$","div",null,{"hidden":true,"children":["$","$Lc",null,{"children":["$","$3",null,{"name":"Next.Metadata","children":"$@d"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$e",[]],"s":false,"S":true}
f:I[78995,["/SDGF/_next/static/chunks/480617ecc94242dd.js","/SDGF/_next/static/chunks/34ee24179ca0df5c.js","/SDGF/_next/static/chunks/3708046326082a93.js","/SDGF/_next/static/chunks/3c1c2c6d31fa05ec.js"],"AppendixDetail"]
10:T2d02,
# **Appendix 7: De-Identification Techniques and Evaluation of Privacy in Synthetic Data**

## **De-identification Techniques**

De-identification aims to break the link between a dataset and an individual in the real world, ensuring that a disclosed fact (e.g., “a patient is being treated for HIV”) cannot be linked back to an identified person.

The harm being prevented is **identity disclosure**, which occurs when data is re-identified. Identity disclosure can arise by:

1. **Matching a person to data**, or
2. **Matching data to a person**

Robustness checks should test for both types of re-identification risk.

> “De-identification is not a single technique, but a collection of approaches, algorithms, and tools… privacy protection improves as more aggressive techniques are employed, but data utility decreases.”

There is no single correct method; de-identification is a **risk management exercise**. The effectiveness of different techniques depends on:

* The type of data
* Context
* Re-identification threats
* Desired utility

Examples of de-identification techniques include:

* **Aggregation**
* **Suppression** (removing identifiers or overtly identifying fields)
* **Generalisation** (e.g., replacing date of birth with an age band)
* **Pseudonymisation** (SLKs, encryption, hashing)
* **Perturbation** (noise addition, micro-aggregation, data swapping)

---

# **Evaluation of Privacy in Synthetic Data**

## **Introduction**

Privacy protection is the first principle guiding synthetic data generation, even though synthetic data represents a **trade-off** between fidelity, utility, and privacy.

Residual privacy risks persist due to factors such as:

* Model overfitting
* Memorisation of sensitive patterns
* Inference attacks exploiting preserved distributions

Some generative models implement privacy mechanisms (DPGAN, PATEGAN, ADSGAN), but **privacy evaluation is still essential**.

This appendix provides a conceptual overview of privacy evaluation methods — not algorithms or code.

There is no universal definition of “privacy risk,” and metrics only capture partial aspects of vulnerability. No single score indicates overall privacy safety.

A wide range of privacy metrics has been proposed. A practical grouping is summarised in **Table 1**. Categories include:

* Traditional re-identifiability metrics
* Distance-based methods
* Adversarial attack-based methods

There is still **no unified standard** for what constitutes adequate privacy protection.

---

# **Table 1: Categories of Metrics for Evaluating Privacy in Synthetic Data**

### **Evaluation Category → Evaluation Method / Metric → Description**

---

## **I. Non-Adversarial Metrics**

### **A. Re-identifiability Metrics**

| Method          | Description                                                                                       |
| --------------- | ------------------------------------------------------------------------------------------------- |
| **k-Anonymity** | Checks if each record is indistinguishable from at least *k–1* others based on quasi-identifiers. |
| **l-Diversity** | Extends k-Anonymity by enforcing at least *l* distinct sensitive values in each group.            |
| **t-Closeness** | Requires each group’s sensitive attribute distribution to be close to the overall distribution.   |

---

### **B. Memorisation Metrics**

| Method                                                | Description                                                                                                            |
| ----------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| **Hitting Rate / Common Row Proportion**              | Measures exact matches between synthetic and source data.                                                              |
| **Close Value Ratio**                                 | Measures “blurry matches” within a set distance threshold.                                                             |
| **Similarity Ratio (ε-identifiability)**              | Tests if fewer than ε proportion of synthetic records are too similar to originals.                                    |
| **Nearest Neighbour Accuracy (Adversarial Accuracy)** | Measures whether records in the original dataset are closest to synthetic vs original points. 0.5 = indistinguishable. |

---

### **C. Distinguishability Metrics**

| Method              | Description                                                                                                    |
| ------------------- | -------------------------------------------------------------------------------------------------------------- |
| **Data Likelihood** | Measures likelihood that synthetic records belong to the original data distribution (Bayesian networks, GMMs). |
| **Detection Rate**  | Measures how well a classifier can distinguish synthetic from real data.                                       |

---

## **II. Adversarial Metrics (Attack-Based)**

### **A. Singling Out Attacks**

| Method                        | Description                                            |
| ----------------------------- | ------------------------------------------------------ |
| **Univariate Singling Out**   | Examines uniqueness in a single attribute.             |
| **Multivariate Singling Out** | Examines uniqueness across combinations of attributes. |

---

### **B. Record Linkage Attacks**

| Method                       | Description                                                        |
| ---------------------------- | ------------------------------------------------------------------ |
| **Public–Public Linkage**    | Uses synthetic data as a bridge between two public datasets.       |
| **Public–Synthetic Linkage** | Links synthetic rows to external datasets using quasi-identifiers. |

---

### **C. Attribute Inference Attacks (AIA)**

| Method                     | Description                                                       |
| -------------------------- | ----------------------------------------------------------------- |
| **Exact Match AIA**        | Infers missing attribute via exact QI matching.                   |
| **Closest Distance AIA**   | Infers sensitive values by nearest synthetic neighbour (k=1).     |
| **Nearest Neighbours AIA** | Uses k>1 nearest neighbours.                                      |
| **ML Inference AIA**       | Trains a model on synthetic data to predict sensitive attributes. |

---

### **D. Membership Inference Attacks (MIA)**

| Method                         | Description                                                                  |
| ------------------------------ | ---------------------------------------------------------------------------- |
| **Closest Distance MIA**       | Membership inferred by proximity of target record to synthetic distribution. |
| **Nearest Neighbours MIA**     | Extends above with k>1 neighbours.                                           |
| **Probability Estimation MIA** | Hypothesis testing of membership probability.                                |
| **Shadow Model MIA**           | Trains classifiers on shadow models with and without the target record.      |

---

# **Consensus-Based Recommendations (R1–R10)**

A Delphi process (13 experts, 3 rounds) produced 10 consensus recommendations:

1. **Base evaluations on Quasi-Identifiers**
2. **Evaluate all records**
3. **Avoid stand-alone similarity metrics**
4. **Align membership disclosure with threat models**
5. **Report prevalence-adjusted scores**
6. **Limit attribute disclosure to members**
7. **Use non-member baselines**
8. **Apply dual thresholds (absolute + relative)**
9. **Validate Differential Privacy empirically**
10. **Report stochastic variation** across multiple synthetic datasets

---

# **Fundamental Concepts**

## **Identity Disclosure**

Occurs when a synthetic record can be linked to a specific individual. Often less relevant for synthetic data because records are artificial, but overfitting may still leak identity.

## **Membership Disclosure**

Occurs when an adversary can tell whether a person’s data was included in the training set.

Useful metrics:

* **F1 score**
* **F naïve** baseline (expected success by random guessing)

## **Attribute Disclosure**

Occurs when an adversary can infer sensitive attributes about an individual using synthetic data.

## **Differential Privacy**

Defines privacy guarantees of the *process*, not the dataset.

Key components:

* **ε (epsilon): privacy budget**
* Lower ε = stronger privacy

---

# **Common Misconceptions to Avoid**

1. **Synthetic data is inherently private**
2. **Record-level similarity indicates privacy risk**
3. **All attributes should be considered in evaluation**
4. **Large DP budgets automatically ensure privacy**

---

# **Practical Evaluation Guides**

## **Guide: Membership Disclosure Evaluation**

* Define threat model
* Calculate naive baseline
* Compute F1 and compare to baseline
* Report assumptions clearly

## **Guide: Attribute Disclosure Evaluation**

* Select quasi-identifiers
* Use non-member baseline
* Compute absolute and relative risks
* Apply dual thresholds

## **Evaluating Multiple Synthetic Datasets**

* Generate **≥10 datasets**
* Report means, SDs, worst cases

---

# **Implementation Considerations**

## Computational Efficiency

* Start with domain-informed QIs
* Expand if needed

## Threshold Determination

* Must be context-specific
* Evidence-informed
* Document assumptions

## Reporting Requirements

Include:

* Absolute & relative metrics
* Variation measures
* Worst-case values
* Threat model definitions

---

# **Future Directions**

* Empirical threshold validation
* Standardised ε interpretation
* Automated privacy evaluation tools
* Joint utility–privacy optimisation
* Methods for non-tabular data

---

# **Conclusion**

Privacy evaluation in synthetic data requires:

* Rigorous, contextual, evidence-based methods
* Clear threat models
* Empirical validation
* Multiple complementary metrics

Perfect privacy is impossible, but **risk can be meaningfully managed** through structured assessment.

---

# **References**

*(All references included exactly as in the source document)*

1. Xie et al., *Differentially Private GANs* — arXiv:1802.06739
2. Jordon et al., *PATE-GAN* — ICLR
3. Yoon et al., *ADS-GAN* — IEEE JBHI
4. Trudslev et al., *Review of Privacy Metrics* — arXiv:2507.11324
5. Osorio-Marulanda et al., *Privacy Mechanisms* — IEEE Access
6. Liao et al., *Pick Your Enemy*
7. Folz et al., *Scoring System for Privacy* — IEEE Access
8. Hernandez et al., *Comprehensive Framework* — Frontiers Digital Health
9. Yan et al., *Benchmarking Synthetic EHR Models* — Nature Communications
10. Pierce et al., *Practical Steps for Implementing Privacy* — WMHP
11. Pilgram et al., *Consensus Privacy Metrics Framework* — Patterns

---

# **Further Resources**

* HealthStats NSW — Privacy and small counts
* CSIRO & OAIC — *De-identification Decision-Making Framework*
* OVIC — *Limitations of De-identification*
* OIC QLD — *Managing Re-identification Risk*
* ISO/IEC 27559:2022 — Privacy enhancing de-identification
* ISO/IEC 27554:2024 — Identity-related risk assessment
* ISO/TS 14265:2024 — Processing purposes classification
* ISO 25237:2017 — Pseudonymisation

7:["$","$Lf",null,{"appendix":{"id":"appendix7","number":7,"title":"Appendix 7 - De-identification Techniques and Privacy Evaluation","purpose":"Reference guidance on de-identification methods and how to evaluate re-identification risks in synthetic data projects.","template":false,"description":"$undefined","body":"$10","component":"ReadOnlyContent","type":"read","exportKey":"appendix7_deid_reference","sections":"$undefined","nodes":"$undefined","terms":"$undefined"}}]
b:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
11:I[27201,["/SDGF/_next/static/chunks/ff1a16fafef87110.js","/SDGF/_next/static/chunks/247eb132b7f7b574.js"],"IconMark"]
d:[["$","title","0",{"children":"SynD - Synthetic Health Data Governance Framework"}],["$","meta","1",{"name":"description","content":"A comprehensive framework for safely generating and using synthetic health data in Australia"}],["$","meta","2",{"name":"generator","content":"v0.app"}],["$","link","3",{"rel":"shortcut icon","href":"/favicon.png"}],["$","link","4",{"rel":"icon","href":"/favicon.png"}],["$","link","5",{"rel":"apple-touch-icon","href":"/favicon.png"}],["$","$L11","6",{}]]
9:null
